#!/bin/bash
# ğŸ§ª Test Runner Script for GitHub Actions
# Comprehensive test suite execution with reporting

set -e

echo "ğŸ§ª Running comprehensive test suite for Online Bookstore Flask App"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_status() { echo -e "${BLUE}[INFO]${NC} $1"; }
print_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
print_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Test configuration
TEST_COVERAGE_THRESHOLD=80
JUNIT_XML_PATH="test-results/junit.xml"
COVERAGE_XML_PATH="coverage.xml"
COVERAGE_HTML_PATH="htmlcov"

# Create test results directory
mkdir -p test-results

print_status "Starting test execution..."

# Run unit tests with coverage
print_status "Running unit tests with coverage..."
pytest \
    --verbose \
    --tb=short \
    --cov=. \
    --cov-report=xml:${COVERAGE_XML_PATH} \
    --cov-report=html:${COVERAGE_HTML_PATH} \
    --cov-report=term-missing \
    --cov-fail-under=${TEST_COVERAGE_THRESHOLD} \
    --junit-xml=${JUNIT_XML_PATH} \
    test_app_model.py \
    test_app_routes.py \
    test_category_edge_cases.py \
    test_integration.py \
    test_integration_full_mid_module.py

print_success "Unit tests completed successfully!"

# Run performance tests
if [ -f "performance_test.py" ]; then
    print_status "Running performance tests..."
    pytest performance_test.py --benchmark-only --benchmark-json=test-results/benchmark.json
    print_success "Performance tests completed!"
else
    print_warning "Performance test file not found, skipping..."
fi

# Generate test summary
print_status "Generating test summary..."
cat > test-results/summary.md << EOF
# ğŸ§ª Test Results Summary

## Coverage Report
- **Coverage Threshold:** ${TEST_COVERAGE_THRESHOLD}%
- **Coverage Report:** [HTML Report](${COVERAGE_HTML_PATH}/index.html)
- **Coverage XML:** ${COVERAGE_XML_PATH}

## Test Files Executed
- âœ… test_app_model.py - Model unit tests
- âœ… test_app_routes.py - Route unit tests  
- âœ… test_category_edge_cases.py - Edge case tests
- âœ… test_integration.py - Integration tests
- âœ… test_integration_full_mid_module.py - Full integration tests
- âœ… performance_test.py - Performance benchmarks

## Artifacts Generated
- JUnit XML: ${JUNIT_XML_PATH}
- Coverage XML: ${COVERAGE_XML_PATH}
- Coverage HTML: ${COVERAGE_HTML_PATH}/
- Benchmark JSON: test-results/benchmark.json

---
*Generated by GitHub Actions Test Runner*
EOF

print_success "Test summary generated: test-results/summary.md"
print_success "All tests completed successfully! âœ…"

# Display coverage summary
if [ -f "${COVERAGE_XML_PATH}" ]; then
    print_status "Coverage Summary:"
    python -c "
import xml.etree.ElementTree as ET
tree = ET.parse('${COVERAGE_XML_PATH}')
root = tree.getroot()
coverage = root.get('line-rate')
print(f'Overall Coverage: {float(coverage)*100:.1f}%')
"
fi